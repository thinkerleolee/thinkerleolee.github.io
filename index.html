<!DOCTYPE html>












  


<html class="theme-next gemini use-motion" lang="">
<head><meta name="generator" content="Hexo 3.8.0">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=6.7.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=6.7.0">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=6.7.0">


  <link rel="mask-icon" href="/images/logo.svg?v=6.7.0" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '6.7.0',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="Thinkerleo">
<meta property="og:url" content="https://thinkerleolee.github.io/index.html">
<meta property="og:site_name" content="Thinkerleo">
<meta property="og:locale" content="default">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Thinkerleo">






  <link rel="canonical" href="https://thinkerleolee.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Thinkerleo</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="default">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Thinkerleo</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home menu-item-active">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/tags" rel="section"><i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/categories" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-commonweal">

    
    
    
      
    

    

    <a href="/404/" rel="section"><i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>Commonweal 404</a>

  </li>

      
      
    </ul>
  

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
            

          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/28/深度学习：神经网络参数的反向传播算法/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/28/深度学习：神经网络参数的反向传播算法/" class="post-title-link" itemprop="url">深度学习：神经网络参数的反向传播算法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-28 10:20:07 / Modified: 11:06:43" itemprop="dateCreated datePublished" datetime="2019-01-28T10:20:07+08:00">2019-01-28</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li><p><strong>神经网络模型的代价函数是什么？</strong></p>
<p>  假设神经网络的训练样本有$m$个</p>
<p>  每个包含一组输入$x$和一组输出信号$y$</p>
<p>  $L$表示神经网络层数</p>
<p>  $S_I$表示每层的neuron个数($S_l$表示输出层神经元个数)</p>
<p>  $S_L$代表最后一层中处理单元的个数。</p>
<p>  将神经网络的分类定义为两种情况：二类分类和多类分类，</p>
<p>  二类分类：$S_L=0, y=0, or, 1$表示哪一类；</p>
<p>  $K$类分类：$S_L=k, y_i = 1$表示分到第$i$类；$(k&gt;2)$</p>
</li>
</ul>
<pre><code>在逻辑回归中，代价函数为：

![逻辑回归-代价函数.png](https://i.loli.net/2019/01/28/5c4e7148aeaab.png)

在逻辑回归中，我们只有一个输出变量，又称为标量，也只有一个因变量$y$。

在神经网络中，有多个输出变量，我们的$h_\theta(x)$是一个维度为$K$的向量。训练集中的因变量也是同样维度的一个量，因此代价函数会更为复杂：

![神经网络-代价函数.png](https://i.loli.net/2019/01/28/5c4e7148e26b9.png)

对于每一行特征，我们都会给出$K$个预测，我们可以利用循环，对于每一行特征都预测$K$个不同结果，然后利用循环在$K$个预测中选择可能性最高的那个，然后与$y$中的实际数据进行比较。

正则化的那一项只是排除了每一层$\theta_0​$后，每一层的$\theta​$ 矩阵的和。最里层的循环$j​$循环所有的行（由$s_l​$ +1 层的激活单元数决定），循环$i​$则循环所有的列，由该层（$s_l​$层）的激活单元数所决定。即：$h_\theta(x)​$与真实值之间的距离为每个样本-每个类输出的加和，对参数进行regularization的bias项处理所有参数的平方和。

![神经网络-代价函数推导.png](https://i.loli.net/2019/01/28/5c4e71496f005.png)
</code></pre><ul>
<li><p><strong>什么是反向传播算法？</strong></p>
<p>  反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，<strong>是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法</strong>。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。</p>
<p>  之前我们在计算神经网络预测结果的时候采用了一种正向传播方法：<br>  从第一层开始一层一层计算，直到最后一层$h_\theta(x)$</p>
<p>  现在，我们为了计算代价函数中的偏导数$\frac{\partial}{\partial\Theta^{(l)}_{ij} }J\left(\Theta\right)$，我们需要采用这种反向传播算法。</p>
<p>  首先，我们要计算最后一层的误差，然后一层层地反向求出各层的误差，直到隐藏层的第一层。</p>
<p>  举个栗子：</p>
<p>  假设我们训练集数量$m=1$，样本为$(x^{1},y^{1})$我们的神经网络是一个四层的神经网络，其$K=4,S_L=4,L=4$</p>
<p>  图示:</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e714970723.png" alt="神经网络-向前传播.png"></p>
<p>  训练样本向前传播以后，我们从最后一层的误差开始计算，误差是激活单元的预测值$(a^{(4)})$与实际值$(y^{k})$之间的误差（$k=1:k$）。我们用$\delta$ 来表示误差。</p>
<p>  则： $\delta^{(4)}=a^4-y$,我们算出了最后一层的误差向量$\delta ^{(4)}$。</p>
<p>  然后我们利用最后一层的误差向量$\delta ^{(4)}$来计算前一层的误差$\delta^{(3)}=\left({\Theta^{(3)} }\right)^{T}\delta^{(4)}\ast g’\left(z^{(3)}\right)$其中 $g’(z^{(3)})$是 $Sigmod$ 函数的导数，$g’(z^{(3)})=a^{(3)}\ast(1-a^{(3)})$。而$(θ^{(3)})^{T}\delta^{(4)}$则是权重导致的误差的和。</p>
<p>  下一步是继续计算第二层的误差： $ \delta^{(2)}=(\Theta^{(2)})^{T}\delta^{(3)}\ast g’(z^{(2)})$ 因为第一层是输入变量，不存在误差。我们有了所有的误差的表达式后，便可以计算代价函数的偏导数了，假设$λ=0$，即我们不做任何正则化处理时有： $\frac{\partial}{\partial\Theta_{ij}^{(l)} }J(\Theta)=a_{j}^{(l)} \delta_{i}^{l+1}$</p>
<p>  重要的是清楚地知道上面式子中上下标的含义：</p>
<p>  $l$ 代表目前所计算的是第几层。</p>
<p>  $j$ 代表目前计算层中的激活单元的下标，也将是下一层的第$j$个输入变量的下标。</p>
<p>  $i$ 代表下一层中误差单元的下标，是受到权重矩阵中第$i$行影响的下一层中的误差单元的下标。</p>
<p>  如果我们考虑正则化处理，并且我们的训练集是一个特征矩阵而非向量。在上面的特殊情况中，我们需要计算每一层的误差单元来计算代价函数的偏导数。在更为一般的情况中，我们同样需要计算每一层的误差单元，但是我们需要为整个训练集计算误差单元，此时的误差单元也是一个矩阵，我们<strong>用$\Delta^{(l)}_{ij}$来表示这个误差矩阵</strong>。<strong>第 $l$ 层的第 $i$ 个激活单元受到第 $j$ 个参数影响而导致的误差</strong>。</p>
<p>  我们的算法表示为：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e71493b34f.png" alt="神经网络-反向传播表示.png"></p>
<p>  即首先用正向传播方法计算出每一层的激活单元，利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播法计算出直至第二层的所有误差。</p>
<p>  在求出了$\Delta_{ij}^{(l)}$之后，我们便可以计算代价函数的偏导数了，计算方法如下： $D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\Theta_{ij}^{(l)}$ ${if}; j \neq 0$</p>
<p>  $D_{ij}^{(l)} :=\frac{1}{m}\Delta_{ij}^{(l)}$ ${if}; j = 0$</p>
<p>  在Octave 中，如果我们要使用 fminuc这样的优化算法来求解求出权重矩阵，我们需要将矩阵首先展开成为向量，在利用算法求出最优解后再重新转换回矩阵。</p>
<p>  假设我们有三个权重矩阵，Theta1，Theta2 和 Theta3，尺寸分别为 10<em>11，10</em>11 和1*11， 下面的代码可以实现这样的转换：</p>
  <figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">thetaVec = [Theta1(:) ; Theta2(:) ; Theta3(:)]</span><br><span class="line"></span><br><span class="line">...optimization using functions like fminuc...</span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(thetaVec(<span class="number">1</span>:<span class="number">110</span>, <span class="number">10</span>, <span class="number">11</span>);</span><br><span class="line"></span><br><span class="line">Theta2 = <span class="built_in">reshape</span>(thetaVec(<span class="number">111</span>:<span class="number">220</span>, <span class="number">10</span>, <span class="number">11</span>);</span><br><span class="line"></span><br><span class="line">Theta1 = <span class="built_in">reshape</span>(thetaVec(<span class="number">221</span>:<span class="number">231</span>, <span class="number">1</span>, <span class="number">11</span>);</span><br></pre></td></tr></table></figure>
</li>
</ul>
<ul>
<li><p><strong>如何初始化所有参数呢？</strong></p>
<p>  任何优化算法都需要一些初始的参数。到目前为止我们都是初始所有参数为0，这样的初始方法对于逻辑回归来说是可行的，但是对于神经网络来说是不可行的。如果我们令所有的初始参数都为0，这将意味着我们第二层的所有激活单元都会有相同的值。同理，如果我们初始所有的参数都为一个非0的数，结果也是一样的。</p>
<p>  我们通常初始参数为正负ε之间的随机值，假设我们要随机初始一个尺寸为10×11的参数矩阵，代码如下：</p>
  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Theta1 = rand(10, 11) * (2*eps) – eps</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>什么是使用神经网络的综合步骤？</strong></p>
<p>  总结一下：</p>
<p>  网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。</p>
<p>  第一层的单元数即我们训练集的特征数量。</p>
<p>  最后一层的单元数是我们训练集的结果的类的数量。</p>
<p>  如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。</p>
<p>  我们真正要决定的是隐藏层的层数和每个中间层的单元数。</p>
<ul>
<li><p>训练神经网络：</p>
<p>  1、参数的随机初始化</p>
<p>  2、利用正向传播方法计算所有的$h_{\theta}(x)$</p>
<p>  3、编写计算代价函数 $J$ 的代码</p>
<p>  4、利用反向传播方法计算所有偏导数</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e71493676c.png" alt="神经网络-循环训练神经网络.png"></p>
<p>  5、利用数值检验方法检验这些偏导数</p>
<p>  6、使用优化算法来最小化代价函数</p>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/28/深度学习：神经网络基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/28/深度学习：神经网络基础/" class="post-title-link" itemprop="url">深度学习：神经网络基础</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-28 10:10:07 / Modified: 11:15:59" itemprop="dateCreated datePublished" datetime="2019-01-28T10:10:07+08:00">2019-01-28</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/深度学习/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="非线性假设"><a href="#非线性假设" class="headerlink" title="非线性假设"></a>非线性假设</h2><ul>
<li><p><strong>线性回归和逻辑回归模型的缺点是什么,为什么需要神经网络?</strong></p>
<p>  我们之前学的，无论是线性回归还是逻辑回归都有这样一个缺点，即：当特征太多时，计算的负荷会非常大。</p>
<p>  下面是一个例子：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b3b30da.png" alt="神经网络-线性模型缺点.png"></p>
<p>  当我们使用x1, x2 的多次项式进行预测时，我们可以应用的很好。 之前我们已经看到过，使用非线性的多项式项，能够帮助我们建立更好的分类模型。假设我们有非常多的特征，例如大于100个变量，我们希望用这100个特征来构建一个非线性的多项式模型，结果将是数量非常惊人的特征组合，即便我们只采用两两特征的组合$(x_1x_2+x_1x_3+x_1x_4+…+x_2x_3+x_2x_4+…+x_{99}x_{100})$，我们也会有接近5000个组合而成的特征。这对于一般的逻辑回归来说需要计算的特征太多了。</p>
<p>  假使我们采用的都是50x50像素的小图片，并且我们将所有的像素视为特征，则会有 2500个特征，如果我们要进一步将两两特征组合构成一个多项式模型，则会有约$2500^2/2$个（接近3百万个）特征。普通的逻辑回归模型，不能有效地处理这么多的特征，这时候我们需要更适合的机器学习算法—-<strong>神经网络</strong>。</p>
</li>
<li><p><strong>神经网络的基本结构有哪些？</strong></p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b398b21.png" alt="神经网络-人工神经网络类型.png"></p>
<p>  <strong>注：双隐层及以上的区域决策类型均能表达任意形状</strong></p>
</li>
<li><p><strong>神经网络的模型怎么表示？</strong></p>
<p>  为了构建神经网络模型，我们需要首先思考大脑中的神经网络是怎样的？每一个神经元都可以被认为是一个处理单元/神经核（processing unit/Nucleus），它含有许多<strong>输入/树突（input/Dendrite）</strong>，并且有一个<strong>输出/轴突（output/Axon）</strong>。神经网络是大量神经元相互链接并通过电脉冲来交流的一个网络。</p>
<p>  如图：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b3b9933.png" alt="神经网络-生物神经模型.png"></p>
<p>  其中：</p>
<p>  <strong>树突</strong> 对应 <strong>数据输入</strong></p>
<p>  <strong>轴突</strong> 对应 <strong>数据输出</strong></p>
<p>  神经网络模型建立在很多神经元之上，每一个神经元又是一个个学习模型。这些神经元（也叫激活单元，activation unit）采纳一些特征作为输出，并且根据本身的模型提供一个输出。下图是一个以逻辑回归模型作为自身学习模型的神经元示例，在神经网络中，参数又可被成为<strong>权重（weight）</strong>。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b3b52b4.png" alt="神经网络-权重表示.png"></p>
<p>  人们在人类神经结构的基础上，发明了一种类似神经元仿生的模型：人工神经网络</p>
<p>  如图：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b3b7680.png" alt="神经网络-人工神经网络模型.png"></p>
<p>  其中$x_1, x_2, x_3$是输入单元（input units），我们将<strong>原始数据</strong>输入给它们。 $a_1, a_2, a_3$是中间单元，它们负责将数据<strong>进行处理</strong>，然后呈<strong>递到下一层</strong>。 最后是输出单元，它<strong>负责计算</strong>$h_θ(x)$。</p>
<p>  下面引入一些标记法来帮助描述模型： $a_{i}^{\left( j \right)}$ 代表第$j$ 层的第 $i$ 个激活单元。${ {\theta }^{\left( j \right)} }$代表从第 $j$ 层映射到第$j+1$ 层时的权重的矩阵，例如${ {\theta }^{\left( 1 \right)} }$代表从第一层映射到第二层的权重的矩阵。其尺寸为：以第 $j+1$层的激活单元数量为行数，以第 $j$ 层的激活单元数加一为列数的矩阵。例如：上图所示的神经网络中${ {\theta }^{\left( 1 \right)} }$的尺寸为 3*4。</p>
<p>  对于上图所示的模型，激活单元和输出分别表达为：</p>
<script type="math/tex; mode=display">
  a_{1}^{(2)}=g(\Theta _{10}^{(1)}{ {x}{0} }+\Theta _{11}^{(1)}{ {x}{1} }+\Theta _{12}^{(1)}{ {x}{2} }+\Theta _{13}^{(1)}{ {x}{3} })</script><script type="math/tex; mode=display">
  a_{2}^{(2)}=g(\Theta _{20}^{(1)}{ {x}{0} }+\Theta _{21}^{(1)}{ {x}{1} }+\Theta _{22}^{(1)}{ {x}{2} }+\Theta _{23}^{(1)}{ {x}{3} })</script><script type="math/tex; mode=display">
  a_{3}^{(2)}=g(\Theta _{30}^{(1)}{ {x}{0} }+\Theta _{31}^{(1)}{ {x}{1} }+\Theta _{32}^{(1)}{ {x}{2} }+\Theta _{33}^{(1)}{ {x}{3} })</script><script type="math/tex; mode=display">
  { {h}_{\Theta } }(x)=g(\Theta _{10}^{(2)}a{0}^{(2)}+\Theta _{11}^{(2)}a{1}^{(2)}+\Theta _{12}^{(2)}a{2}^{(2)}+\Theta _{13}^{(2)}a{3}^{(2)})</script><p>  相对于使用循环来编码，利用向量化的方法会使得计算更为简便。以上面的神经网络为例，试着计算第二层的值：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e72b3a143c.png" alt="神经网络-向量化计算.png"></p>
<p>  我们令 ${ {z}^{\left( 2 \right)} }={ {\theta }^{\left( 1 \right)} }x$，则 ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$ ，计算后添加 $a_{0}^{\left( 2 \right)}=1$。 计算输出的值为：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4e732dbe83c.png" alt="神经网络-向量计算.png"></p>
<p>  我们令${ {z}^{\left( 3 \right)} }={ {\theta }^{\left( 2 \right)} }{ {a}^{\left( 2 \right)} }$，则 $h_\theta(x)={ {a}^{\left( 3 \right)} }=g({ {z}^{\left( 3 \right)} })$。 这只是针对训练集中一个训练实例所进行的计算。如果我们要对整个训练集进行计算，我们需要将训练集特征矩阵进行转置，使得同一个实例的特征都在同一列里。即：<br>  ${ {z}^{\left( 2 \right)} }={ {\Theta }^{\left( 1 \right)} }\times { {X}^{T} }$</p>
<p>  ${ {a}^{\left( 2 \right)} }=g({ {z}^{\left( 2 \right)} })$</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/28/线性模型：Logistic回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/28/线性模型：Logistic回归/" class="post-title-link" itemprop="url">机器学习算法：Logistic回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-28 00:30:07 / Modified: 01:39:52" itemprop="dateCreated datePublished" datetime="2019-01-28T00:30:07+08:00">2019-01-28</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Logistic回归"><a href="#Logistic回归" class="headerlink" title="Logistic回归"></a>Logistic回归</h2><ul>
<li><p><strong>Logistic回归算法解决了什么问题？公式表示是什么？</strong></p>
<p>  Logistic回归事实上是一种分类算法，因为历史原因起名叫“回归”这两个字可能有点疑惑，但是它是种分类算法。</p>
<p>  以肿瘤是否为恶性来说：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de0563df00.png" alt="逻辑回归-肿瘤-1.png"></p>
<p>  1：是恶性肿瘤</p>
<p>  0：是良性肿瘤</p>
<p>  无论什么情况这个分类问题总是有：$0\leq h_{\theta}(x) \leq 1$</p>
<p>  这个算法的性质是：<strong>它的输出值永远在0到 1 之间。</strong></p>
<p>  所以，我们引入一个新的模型：<strong>逻辑回归模型（Logistic Regression）</strong></p>
<p>  逻辑回归模型的假设是：</p>
<p>   $h_\theta \left( x \right)=g\left(\theta^{T}X \right)$ </p>
<p>   其中： $X$ 代表特征向量 $g$ 代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function），公式为： $g\left( z \right)=\frac{1} {1+{ {e}^{-z} } }$。</p>
<p>  S形函数就是为了将回归模型函数的值域限制在$(0,1)$之间。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de05611f62.png" alt="逻辑回归公式.png"></p>
</li>
<li><p><strong>判定边界是什么？</strong></p>
<p>  <strong>判定边界就是逻辑回归要求的最终目标。</strong></p>
<p>  在逻辑回归中，我们预测：</p>
<p>  当${h_\theta}\left( x \right)&gt;=0.5$时，预测 $y=1$。</p>
<p>  当${h_\theta}\left( x \right)&lt;0.5$时，预测 $y=0$ 。</p>
<p>  根据上面绘制出的 S 形函数图像，我们知道当</p>
<p>  $z=0$ 时 $g(z)=0.5$</p>
<p>  $z&gt;0$ 时 $g(z)&gt;0.5$</p>
<p>  $z&lt;0$ 时 $g(z)&lt;0.5$</p>
<p>  又 $z={\theta^{T} }x$ ，即： ${\theta^{T} }x&gt;=0$ 时，预测 $y=1$ ${\theta^{T} }x&lt;0$ 时，预测 $y=0$</p>
</li>
</ul>
<pre><code>如下图，黄色线就是决策边界：

线性判定边界：

![逻辑回归-决策边界1.png](https://i.loli.net/2019/01/28/5c4de0565cae5.png)

非线性判定边界：

![逻辑回归-决策边界2.png](https://i.loli.net/2019/01/28/5c4de05655765.png)
</code></pre><ul>
<li><p><strong>逻辑回归的代价函数是什么？可以简化么？</strong></p>
<p>  线性回归的代价函数为：$J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{1}{2}{ {\left( {h_\theta}\left({x}^{\left( i \right)} \right)-{y}^{\left( i \right)} \right)}^{2} } }$ 。</p>
<p>  逻辑回归的代价函数为：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de0562d331.png" alt="逻辑回归-代价函数.png"></p>
<p>  ${h_\theta}\left( x \right)$与 $Cost\left( {h_\theta}\left( x \right),y \right)$之间的关系如下图所示：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de05657f02.png" alt="逻辑回归-代价函数图像.png"></p>
<p>  这样构建的$Cost\left( {h_\theta}\left( x \right),y \right)$函数的特点是：</p>
<p>  当实际的 $y=1$ 且${h_\theta}\left( x \right)$也为 1 时误差为 0</p>
<p>  当 $y=1$ 但${h_\theta}\left( x \right)$不为1时误差随着${h_\theta}\left( x \right)$变小而变大；</p>
<p>  当实际的 $y=0$ 且${h_\theta}\left( x \right)$也为 0 时代价为 0</p>
<p>  当$y=0$ 但${h_\theta}\left( x \right)$不为 0时误差随着 ${h_\theta}\left( x \right)$的变大而变大。 </p>
<p>  将构建的 $Cost\left( {h_\theta}\left( x \right),y \right)$简化如下：</p>
<p>  $Cost\left( {h_\theta}\left( x \right),y \right)=-y\times log\left( {h_\theta}\left( x \right) \right)-(1-y)\times log\left( 1-{h_\theta}\left( x \right) \right)$ 带入代价函数得到： $J\left( \theta \right)=\frac{1}{m}\sum\limits_{i=1}^{m}{[-{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)-\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$ </p>
<p>  <strong>即代价函数可以简化为：</strong>$J\left( \theta \right)=-\frac{1}{m}\sum\limits_{i=1}^{m}{[{ {y}^{(i)} }\log \left( {h_\theta}\left( { {x}^{(i)} } \right) \right)+\left( 1-{ {y}^{(i)} } \right)\log \left( 1-{h_\theta}\left( { {x}^{(i)} } \right) \right)]}$</p>
</li>
<li><p><strong>怎么用梯度下降法来求解$\theta？$</strong></p>
<p>  在得到代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：</p>
<p>  Repeat {</p>
<p>  $\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j} J(\theta)$ (simultaneously update all ) </p>
<p>  }</p>
<p>  求导后得到：</p>
<p>  Repeat { </p>
<p>  $\theta_j := \theta_j - \alpha \frac{1}{m}\sum\limits_{i=1}^{m}{ {\left( {h_\theta}\left( \mathop{x}^{\left( i \right)} \right)-\mathop{y}^{\left( i \right)} \right)} }\mathop{x}_{j}^{(i)}$ (simultaneously update all )</p>
<p>  }</p>
<p>  在这个视频中，我们定义了单训练样本的代价函数，凸性分析的内容是超出这门课的范围的，但是可以证明我们所选的代价值函数会给我们一个凸优化问题。代价函数$J(\theta)$会是一个凸函数，并且没有局部最优值。</p>
</li>
<li><p><strong>高级优化的算法？</strong></p>
<p>  一些梯度下降算法之外的选择： 除了梯度下降算法以外，还有一些常被用来令代价函数最小的算法，这些<strong>算法更加复杂和优越</strong>，而且通常不需要人工选择学习率，通常比梯度下降算法要更加快速。</p>
<p>  这些算法有：<strong>共轭梯度（Conjugate Gradient）</strong>，<strong>局部优化法(Broyden fletcher goldfarb shann,BFGS)</strong>和<strong>有限内存局部优化法(LBFGS)</strong> ，fminunc是 matlab和octave 中都带的一个最小值优化函数，使用时我们需要提供代价函数和每个参数的求导。，它们需要有一种方法来计算 $J\left( \theta \right)$，以及需要一种方法计算导数项，然后使用比梯度下降更复杂的算法来最小化代价函数。这三种算法的具体细节超出了本门课程的范畴。实际上你最后通常会花费很多天，或几周时间研究这些算法，你可以专门学一门课来提高数值计算能力，不过让我来告诉你他们的一些特性：</p>
<p>  这三种算法有许多优点：</p>
<p>  一个是使用这其中任何一个算法，你<strong>通常不需要手动选择学习率 $\alpha$</strong>，所以对于这些算法的一种思路是，给出计算导数项和代价函数的方法，你可以认为算法有一个智能的内部循环，而且，事实上，他们确实有一个智能的内部循环，称为线性搜索(line search)算法，它可以自动尝试不同的学习速率 $\alpha$，并自动选择一个好的学习速率 $a$，因此它甚至可以为每次迭代选择不同的学习速率，那么你就不需要自己选择。这些算法实际上在做更复杂的事情，不仅仅是选择一个好的学习速率，所以它们往往最终比梯度下降收敛得快多了，不过关于它们到底做什么的详细讨论，已经超过了本门课程的范围。</p>
</li>
<li><p><strong>当遇到多分类的时候怎么样操作算法？</strong></p>
<p>  使用One-vs-all（一对多）方法可以有效解决这个问题。</p>
<p>  我用3种不同的符号来代表3个类别，问题就是给出3个类型的数据集，我们如何得到一个学习算法来进行分类呢？</p>
<p>  我们现在已经知道如何进行二元分类，可以使用逻辑回归，对于直线或许你也知道，可以将数据集一分为二为正类和负类。用一对多的分类思想，我们可以将其用在多类分类问题上。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de05652feb.png" alt="一对多.png"><br>  现在我们有一个训练集，好比上图表示的有3个类别，我们用三角形表示 $y=1$，方框表示$y=2$，叉叉表示 $y=3$。我们下面要做的就是使用一个训练集，将其分成3个二元分类问题。</p>
<p>  我们先从用三角形代表的类别1开始，实际上我们可以创建一个，新的”伪”训练集，类型2和类型3定为负类，类型1设定为正类，我们创建一个新的训练集，如下图所示的那样，我们要拟合出一个合适的分类器。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4de05660bd9.png" alt="一对多-2.png"></p>
<p>  这里的三角形是正样本，而圆形代表负样本。可以这样想，设置三角形的值为1，圆形的值为0，下面我们来训练一个标准的逻辑回归分类器，这样我们就得到一个正边界。</p>
<p>  为了能实现这样的转变，我们<strong>将多个类中的一个类标记为正向类（$y=1$）</strong>，然后<strong>将其他所有类都标记为负向类</strong>，这个模型记作$h_\theta^{\left( 1 \right)}\left( x \right)$。接着，类似地第我们选择另一个类标记为正向类（$y=2$），再将其它类都标记为负向类，将这个模型记作 $h_\theta^{\left( 2 \right)}\left( x \right)$,依此类推。 最后我们得到一系列的模型简记为： $h_\theta^{\left( i \right)}\left( x \right)=p\left( y=i|x;\theta \right)$其中：$i=\left( 1,2,3….k \right)$</p>
<p>  最后，在我们需要做预测时，我们将所有的分类机都运行一遍，然后对每一个输入变量，都选择最高可能性的输出变量。</p>
<p>  总之，我们已经把要做的做完了，现在要做的就是训练这个逻辑回归分类器：$h_\theta^{\left( i \right)}\left( x \right)$， 其中 $i$ 对应每一个可能的 $y=i$，最后，为了做出预测，我们给出输入一个新的 $x$ 值，用这个做预测。我们要做的就是在我们三个分类器里面输入 $x$，然后我们选择一个让 $h_\theta^{\left( i \right)}\left( x \right)$ 最大的$ i$，即<br>  $\mathop{\max}\limits_i,h_\theta^{\left( i \right)}\left( x \right)$。</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/28/线性模型：单变量线性回归/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/28/线性模型：单变量线性回归/" class="post-title-link" itemprop="url">机器学习算法：单变量线性回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-28 00:10:07 / Modified: 11:14:32" itemprop="dateCreated datePublished" datetime="2019-01-28T00:10:07+08:00">2019-01-28</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/机器学习/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h2><ul>
<li><p><strong>什么是回归分析?</strong></p>
<p>  回归分析（Regression Analysis）是一种统计学上分析数据的方法，目的在于了解两个或多个变量间是否相关、相关方向与强度，并建立数学模型以便观察特定变量来预测研究者感兴趣的变量。更具体的来说，回归分析可以帮助人们了解在只有一个自变量变化时因变量的变化量。一般来说，通过回归分析我们可以由给出的自变量估计因变量的条件期望。</p>
</li>
<li><p><strong>什么是线性回归？</strong></p>
<p>  线性回归（Linear regression）是利用称为线性回归方程的最小平方函數对一个或多个自变量和因变量之间关系进行建模的一种回归分析。 这种函数是一个或多个称为回归系数的模型参数的线性组合。 只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归。</p>
</li>
<li><p><strong>Andrew Ng课程中对于监督学习算法的表示是什么？</strong></p>
<p>  $m$代表训练集中实例的数量</p>
<p>  $x$代表特征/输入变量</p>
<p>  $y$代表目标变量/输出变量</p>
<p>  $(x,y)$代表训练集中的实例</p>
<p>  $(x^{(i)},y^{(i)})$代表第$i$个观察实例</p>
<p>  $h$代表学习算法的解决方案或函数成为假设。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde2865e7b.png" alt="监督学习模型.png"></p>
</li>
<li><p><strong>单变量线性回归的数学表示是什么?</strong></p>
<p>  假设函数：</p>
<script type="math/tex; mode=display">
      h_\theta(x)=\theta_0+\theta_1x</script><p>  其中$\theta_0、\theta_1$是参数</p>
<p>  代价函数：</p>
<script type="math/tex; mode=display">
      J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})^2</script><p>  我们的目的是使得代价函数$J(\theta_0,\theta_1)$取得最小值</p>
<p>  也就是求 $minimize~J(\theta_0, \theta_1)$</p>
</li>
</ul>
<ul>
<li><p><strong>那么。。怎么求$minimize~J(\theta_0, \theta_1)$呢？</strong></p>
<p>  $minimize~J(\theta_0, \theta_1)$的图像，大概是这个样子的：</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde80ca5be.png" alt="代价函数图像.png"></p>
<p>  这时候，我们就要请出<strong>梯度下降</strong>法了。梯度下降在之前数学基础中有相关的内容。</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde809da57.jpg" alt="梯度下降-山.jpg"></p>
<p>  想象一下你正站立在山的这一点上，站立在你想象的公园这座红色山上，在梯度下降算法中，我们要做的就是旋转360度，看看我们的周围，并问自己要在某个方向上，用小碎步尽快下山。这些小碎步需要朝什么方向？如果我们站在山坡上的这一点，你看一下周围，你会发现最佳的下山方向，你再看看周围，然后再一次想想，我应该从什么方向迈着小碎步下山？然后你按照自己的判断又迈出一步，重复上面的步骤，从这个新的点，你环顾四周，并决定从什么方向将会最快下山，然后又迈进了一小步，并依此类推，直到你接近局部最低点的位置。</p>
<p>  <strong>简而言之，就是小碎步下山，每一步都选出下山最快的那个方向迈步子。</strong></p>
<p>  Andrew Ng对梯度下降的算法公式定义是：</p>
<p>  repeat until convergence<br>  {</p>
<script type="math/tex; mode=display">
  \theta_j := \theta_j-\alpha\frac{\alpha}{\alpha\theta_j}J(\theta_0,\theta_1)~~~~(for~j=0~and~j=1)</script><p>  }</p>
<p>  <strong>其中a是学习率（learning rate），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。</strong></p>
<p>  注意在小步迭代中，参数要<strong>同步更新</strong></p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde80c8947.png" alt="梯度下降-同步更新.png"></p>
<p>  对于求 $minimize~J(\theta_0, \theta_1)$，用梯度下降法就是：</p>
<p>  repeat until convergence{</p>
<p>  $\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})$</p>
<p>  $\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{i}$</p>
<p>  }</p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde80e8441.png" alt="代价函数-梯度下降求值.png"></p>
<p>  其中蓝框和红框中的就是对参数求偏微分的结果。</p>
</li>
<li><p><strong>梯度下降为什么会陷入局部最优解的问题？</strong></p>
<p>  <img src="https://i.loli.net/2019/01/28/5c4dde809da57.jpg" alt="梯度下降-山.jpg"></p>
<p>  如图，在有多个局部最优解的时候，有时会得出不同的最优解结论</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/27/视频编解码学习一：编解码理论基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/27/视频编解码学习一：编解码理论基础/" class="post-title-link" itemprop="url">视频编解码学习一：编解码理论基础</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-27 23:40:07" itemprop="dateCreated datePublished" datetime="2019-01-27T23:40:07+08:00">2019-01-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-01-28 11:14:43" itemprop="dateModified" datetime="2019-01-28T11:14:43+08:00">2019-01-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/音视频开发/" itemprop="url" rel="index"><span itemprop="name">音视频开发</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li><p><strong>为什么要进行视频压缩？</strong></p>
<p>  未经压缩的数字视频（原始数据）的数据量巨大导致的存储困难、传输困难。</p>
</li>
<li><p><strong>为什么能进行视频压缩？</strong></p>
<p>  可以去除视频中的冗余信息</p>
<ul>
<li>空间冗余：图像相邻像素之间有较强的相关性</li>
<li>时间冗余：视频序列的相邻图像之间内容相似</li>
<li>编码冗余：不同像素值出现的概率不同</li>
<li>视觉冗余：人的视觉系统对某些细节不敏感</li>
<li>知识冗余：规律性的结构可由先验知识和背景知识得到</li>
</ul>
</li>
<li><p><strong>数据压缩分了几类？</strong></p>
<ul>
<li><p>无损压缩（Lossless）</p>
<ul>
<li><p>压缩前和解压缩后数据完全一致</p>
</li>
<li><p>压缩比比较低(2:1~3:1)</p>
<p>无损压缩算法的代表为:gzip(数据)、APE(音频)、JPEG-LS（视频）</p>
</li>
</ul>
</li>
<li><p>有损压缩（Lossy）</p>
<ul>
<li><p>压缩前后数据不一致$X \neq X’$</p>
</li>
<li><p>压缩比高(10:1~20:1)</p>
</li>
<li><p>例如：MPEG-2，H.264/AVC，AVS</p>
</li>
</ul>
<p><a href="https://i.loli.net/2019/01/27/5c4dce56edafc.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56edafc.png" alt="201207290832356582.png"></a></p>
</li>
<li><p><strong>那什么是编解码器呢？</strong></p>
<ul>
<li><p>编码器（Encoder）</p>
<ul>
<li>压缩信号的设备或程序</li>
</ul>
</li>
<li><p>解码器（Decoder）</p>
<ul>
<li>解压缩信号的设备或程序</li>
</ul>
</li>
<li><p>编解码器（Codec）</p>
<ul>
<li>编解码器对</li>
</ul>
</li>
</ul>
</li>
<li><p>编码器和解码器的一般工作过程是什么？</p>
<ul>
<li><p>编码器</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce5717f84.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce5717f84.png" alt="201207290832366059.png"></a></p>
</li>
<li><p>解码器</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce56eecc3.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56eecc3.png" alt="201207290832364663.png"></a></p>
</li>
</ul>
</li>
</ul>
<ul>
<li><p><strong>视频传输的QoS参数是什么</strong>？</p>
<ul>
<li><p>数据包的端到端的延迟</p>
</li>
<li><p>带宽：比特/秒   </p>
</li>
<li><p>数据包的流失率（丢包率）</p>
</li>
<li><p>数据包的延迟时间的波动</p>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>我们经常见到的.mov\.mp4\.rmvb这是指的是编码标准吗？</strong></p>
<p>  不是，.mov等只是封装格式，封装了视频编码协议、音频编码协议等信息。</p>
<p>  如下图所示：“变形金刚预告片_h1080p.mov”采用MPEG4的QuickTime封装格式（MOV），采用了H.264（AVC）的压缩编码标准。</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce571e18f.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce571e18f.jpg" alt="20140201115933359.jpg"></a></p>
</li>
</ul>
<ul>
<li><p><strong>那网上下载下来的视频播放流程是什么呢？</strong></p>
<p>  视频播放器播放一个互联网上的视频文件，需要经过以下几个步骤：解协议，解封装，解码视音频，视音频同步。如果播放本地文件则不需要解协议，为以下几个步骤：解封装，解码视音频，视音频同步。他们的过程如图所示。</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce56e2eda.jpg" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56e2eda.jpg" alt="20140201120523046.jpg"></a></p>
<p>  <strong>解协议</strong>的作用，就是将流媒体协议的数据，解析为标准的相应的封装格式数据。视音频在网络上传播的时候，常常采用各种流媒体协议，例如HTTP，RTMP，或是MMS等等。这些协议在传输视音频数据的同时，也会传输一些信令数据。这些信令数据包括对播放的控制（播放，暂停，停止），或者对网络状态的描述等。解协议的过程中会去除掉信令数据而只保留视音频数据。例如，采用RTMP协议传输的数据，经过解协议操作后，输出FLV格式的数据。</p>
<p>  <strong>解封装</strong>的作用，就是将输入的封装格式的数据，分离成为音频流压缩编码数据和视频流压缩编码数据。封装格式种类很多，例如MP4，MKV，RMVB，TS，FLV，AVI等等，它的作用就是将已经压缩编码的视频数据和音频数据按照一定的格式放到一起。例如，FLV格式的数据，经过解封装操作后，输出H.264编码的视频码流和AAC编码的音频码流。</p>
<p>  <strong>解码</strong>的作用，就是将视频/音频压缩编码数据，解码成为非压缩的视频/音频原始数据。音频的压缩编码标准包含AAC，MP3，AC-3等等，视频的压缩编码标准则包含H.264，MPEG2，VC-1等等。解码是整个系统中最重要也是最复杂的一个环节。通过解码，压缩编码的视频数据输出成为非压缩的颜色数据，例如YUV420P，RGB等等；压缩编码的音频数据输出成为非压缩的音频抽样数据，例如PCM数据。</p>
<p>  <strong>视音频同步</strong>的作用，就是根据解封装模块处理过程中获取到的参数信息，同步解码出来的视频和音频数据，并将视频音频数据送至系统的显卡和声卡播放出来。</p>
</li>
<li><p><strong>目前主流的视频编码标准有什么呢？</strong></p>
<p>  <img src="https://i.loli.net/2019/01/27/5c4dd01441be8.png" alt="TIM截图20190127233628.png"></p>
<p>  图中H.264等仅仅是编码标准，而不是一个具体的编码器，H.264只是给编码器的实现提供参照用的。类似于C\C++和GCC的关系。</p>
</li>
</ul>
<p><strong>整理自 雷霄骅 的Blog <a href="https://blog.csdn.net/leixiaohua1020" target="_blank" rel="noopener">https://blog.csdn.net/leixiaohua1020</a></strong></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/27/视频编解码学习二：视频表示基础/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/27/视频编解码学习二：视频表示基础/" class="post-title-link" itemprop="url">视频编解码学习二：视频表示基础</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-27 23:30:07 / Modified: 23:56:38" itemprop="dateCreated datePublished" datetime="2019-01-27T23:30:07+08:00">2019-01-27</time>
            

            
              

              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing"><a href="/categories/音视频开发/" itemprop="url" rel="index"><span itemprop="name">音视频开发</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ul>
<li><p><strong>视频的本质是什么？</strong></p>
<p>  视频是连续的图像，视频由多幅图像构成，包含对象的运动信息，又称为运动图像。</p>
</li>
<li><p><strong>视频有哪几种采样？</strong></p>
<p>  视频从时间和空间两个维度进行采样。</p>
<ul>
<li><p>时间采样 </p>
<p>  度量：</p>
<p>  帧率：帧/秒</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce56adf42.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56adf42.png" alt="20190126152938.png"></a></p>
</li>
<li><p>空间采样</p>
<p>  度量：</p>
<p>  解析度/分辨率（Resolution）</p>
<p>  <a href="https://i.loli.net/2019/01/27/5c4dce56ce8ec.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56ce8ec.png" alt="20190126153053.png"></a></p>
</li>
</ul>
</li>
<li><p><strong>什么是人类视觉系统HVS？</strong></p>
<p>  HVS是由一下器官组成的：</p>
<ul>
<li>眼睛</li>
<li>神经</li>
<li><p>大脑</p>
<p>HVS特点：</p>
</li>
<li>对高频信息不敏感</li>
<li>对高对比度更敏感</li>
<li>对亮度信息比色度信息更敏感</li>
<li>对运动的信息更敏感</li>
</ul>
</li>
<li><p><strong>什么是RGB色彩空间？</strong></p>
<ul>
<li><p>三原色：红（R），绿（G），蓝（B）。</p>
<p>任何颜色都可以通过按一定比例混合三原色产生。</p>
<p>RGB色度空间<br>由RGB三原色组成<br>广泛用于BMP，TIFF，PPM等</p>
<p>每个色度成分通常用8bit表示[0,255]/[0,ffff]</p>
<p><a href="https://i.loli.net/2019/01/27/5c4dce56f16be.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56f16be.png" alt="201207290832374305.png"></a></p>
</li>
</ul>
</li>
<li><p><strong>什么是YUV色彩空间？</strong></p>
<ul>
<li>Y：亮度分量，其实Y就是图像的灰度值</li>
<li><p>UV：两个色度分量</p>
<p>YUV更好的反映HVS特点</p>
<p><strong>RGB转化到YUV空间</strong>:</p>
<p><a href="https://i.loli.net/2019/01/27/5c4dce56ceae5.png" target="_blank" rel="noopener"><img src="https://i.loli.net/2019/01/27/5c4dce56ceae5.png" alt="20190126160638.png"></a></p>
</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://thinkerleolee.github.io/2019/01/27/测试博客/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Thinkerleo">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/thinkerleo.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Thinkerleo">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/01/27/测试博客/" class="post-title-link" itemprop="url">测试博客</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-01-27 15:30:07 / Modified: 21:45:32" itemprop="dateCreated datePublished" datetime="2019-01-27T15:30:07+08:00">2019-01-27</time>
            

            
              

              
            
          </span>

          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="一级标题"><a href="#一级标题" class="headerlink" title="一级标题"></a>一级标题</h1><h2 id="二级标题"><a href="#二级标题" class="headerlink" title="二级标题"></a>二级标题</h2><script type="math/tex; mode=display">
    y=ax+b</script>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/thinkerleo.jpg" alt="Thinkerleo">
            
              <p class="site-author-name" itemprop="name">Thinkerleo</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives">
                
                    <span class="site-state-item-count">7</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  <a href="/categories/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">3</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  <a href="/tags/index.html">
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">10</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/thinkerleolee" title="GitHub &rarr; https://github.com/thinkerleolee" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="mailto:thinkerleo@hotmail.com" title="E-Mail &rarr; mailto:thinkerleo@hotmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
            </div>
          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Thinkerleo</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v6.7.0</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=6.7.0"></script>

  <script src="/js/src/motion.js?v=6.7.0"></script>



  
  


  <script src="/js/src/affix.js?v=6.7.0"></script>

  <script src="/js/src/schemes/pisces.js?v=6.7.0"></script>




  

  


  <script src="/js/src/bootstrap.js?v=6.7.0"></script>



  



  





  

  

  

  

  
  

  
  
    
      
        
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
    
      <script type="text/x-mathjax-config">
  

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      
      equationNumbers: {
        autoNumber: "AMS"
      }
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
      for (i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>
<script src="//cdn.jsdelivr.net/npm/mathjax@2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<style>
.MathJax_Display {
  overflow: auto hidden;
}
</style><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->

    
  


  

  

  

  

  

  

  

  

</body>
</html>
